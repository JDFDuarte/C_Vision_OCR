{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from PIL import Image\n",
    "from itertools import groupby\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load EMNIST Dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'emnist/balanced',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and preprocess the dataset\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, [28, 28])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = ds_info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "882/882 - 62s - 70ms/step - accuracy: 0.7620 - loss: 0.7639 - val_accuracy: 0.8592 - val_loss: 0.4289 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "882/882 - 60s - 68ms/step - accuracy: 0.8599 - loss: 0.4074 - val_accuracy: 0.8755 - val_loss: 0.3699 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "882/882 - 59s - 67ms/step - accuracy: 0.8760 - loss: 0.3545 - val_accuracy: 0.8774 - val_loss: 0.3519 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "882/882 - 54s - 61ms/step - accuracy: 0.8829 - loss: 0.3286 - val_accuracy: 0.8871 - val_loss: 0.3423 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "882/882 - 53s - 60ms/step - accuracy: 0.8881 - loss: 0.3125 - val_accuracy: 0.8875 - val_loss: 0.3435 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "882/882 - 61s - 69ms/step - accuracy: 0.8925 - loss: 0.3021 - val_accuracy: 0.8883 - val_loss: 0.3434 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "882/882 - 71s - 80ms/step - accuracy: 0.8940 - loss: 0.2952 - val_accuracy: 0.8896 - val_loss: 0.3327 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "882/882 - 56s - 64ms/step - accuracy: 0.8952 - loss: 0.2925 - val_accuracy: 0.8914 - val_loss: 0.3467 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "882/882 - 60s - 68ms/step - accuracy: 0.8971 - loss: 0.2889 - val_accuracy: 0.8859 - val_loss: 0.3855 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "882/882 - 58s - 66ms/step - accuracy: 0.8983 - loss: 0.2855 - val_accuracy: 0.8924 - val_loss: 0.3549 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "882/882 - 57s - 65ms/step - accuracy: 0.8985 - loss: 0.2832 - val_accuracy: 0.8920 - val_loss: 0.3852 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "882/882 - 56s - 63ms/step - accuracy: 0.8993 - loss: 0.2838 - val_accuracy: 0.8888 - val_loss: 0.3690 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "882/882 - 57s - 64ms/step - accuracy: 0.9005 - loss: 0.2837 - val_accuracy: 0.8907 - val_loss: 0.3685 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "882/882 - 60s - 68ms/step - accuracy: 0.9082 - loss: 0.2534 - val_accuracy: 0.8953 - val_loss: 0.3549 - learning_rate: 5.0000e-04\n",
      "Epoch 15/20\n",
      "882/882 - 68s - 77ms/step - accuracy: 0.9103 - loss: 0.2490 - val_accuracy: 0.8942 - val_loss: 0.3376 - learning_rate: 5.0000e-04\n",
      "Epoch 16/20\n",
      "882/882 - 63s - 72ms/step - accuracy: 0.9103 - loss: 0.2497 - val_accuracy: 0.8952 - val_loss: 0.3574 - learning_rate: 5.0000e-04\n",
      "Epoch 17/20\n",
      "882/882 - 58s - 66ms/step - accuracy: 0.9105 - loss: 0.2469 - val_accuracy: 0.8971 - val_loss: 0.3199 - learning_rate: 5.0000e-04\n",
      "Epoch 18/20\n",
      "882/882 - 60s - 68ms/step - accuracy: 0.9106 - loss: 0.2476 - val_accuracy: 0.8949 - val_loss: 0.3204 - learning_rate: 5.0000e-04\n",
      "Epoch 19/20\n",
      "882/882 - 57s - 65ms/step - accuracy: 0.9111 - loss: 0.2480 - val_accuracy: 0.8948 - val_loss: 0.3345 - learning_rate: 5.0000e-04\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "882/882 - 57s - 64ms/step - accuracy: 0.9106 - loss: 0.2457 - val_accuracy: 0.8956 - val_loss: 0.3447 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 2. Model Building\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(5,5), padding=\"Same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=(5,5), padding=\"Same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding=\"Same\", activation=\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding=\"Same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))  # Adjusted for EMNIST classes\n",
    "\n",
    "# Compile the model\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Learning rate reduction\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor=\"val_accuracy\", patience=3, verbose=1, factor=0.5, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=ds_test,\n",
    "    verbose=2,\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"emnist_recognition_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Training data shape: (112800, 28, 28, 1)\n",
      "Test data shape: (18800, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "from emnist import extract_training_samples, extract_test_samples\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def load_emnist_data():\n",
    "    try:\n",
    "        # Extract training and test samples from EMNIST dataset\n",
    "        X_train, y_train = extract_training_samples('balanced')\n",
    "        X_test, y_test = extract_test_samples('balanced')\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        X_train = X_train.astype('float32') / 255.0\n",
    "        X_test = X_test.astype('float32') / 255.0\n",
    "        \n",
    "        # Reshape data for CNN input\n",
    "        X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "        X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "        \n",
    "        # Convert labels to categorical format\n",
    "        y_train = to_categorical(y_train, num_classes=47)\n",
    "        y_test = to_categorical(y_test, num_classes=47)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the EMNIST dataset: {e}\")\n",
    "        print(\"Please ensure you have manually downloaded the dataset as instructed.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Load the data\n",
    "X_train, X_test, y_train, y_test = load_emnist_data()\n",
    "\n",
    "if X_train is not None:\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "else:\n",
    "    print(\"Failed to load the dataset. Please check the error message above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prediction Function\n",
    "def predict_equation(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    w, h = image.size\n",
    "    new_h = 28\n",
    "    new_w = int((w / h) * 28)\n",
    "    image = image.resize((new_w, new_h))\n",
    "    image_arr = np.array(image)\n",
    "    image_arr = 255 - image_arr\n",
    "    image_arr = image_arr / 255.0\n",
    "\n",
    "    # Split image into individual characters\n",
    "    m = image_arr.any(0)\n",
    "    chars = [image_arr[:,[*g]] for k, g in groupby(np.arange(len(m)), lambda x: m[x] != 0) if k]\n",
    "\n",
    "    # Preprocess each character\n",
    "    char_arrays = []\n",
    "    for char in chars:\n",
    "        width = char.shape[1]\n",
    "        filler = (28 - width) // 2\n",
    "        char_padded = np.pad(char, ((0,0), (filler, 28-width-filler)), mode='constant')\n",
    "        char_arrays.append(char_padded.reshape(28, 28, 1))\n",
    "\n",
    "    char_arrays = np.array(char_arrays)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(char_arrays)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return predicted_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Equation Evaluation\n",
    "def evaluate_equation(predicted_classes):\n",
    "    operators = {10: \"/\", 11: \"+\", 12: \"-\", 13: \"*\"}\n",
    "    equation = \"\"\n",
    "    for cls in predicted_classes:\n",
    "        if cls < 10:\n",
    "            equation += str(cls)\n",
    "        else:\n",
    "            equation += operators[cls]\n",
    "    \n",
    "    try:\n",
    "        result = eval(equation)\n",
    "        return f\"{equation} = {result}\"\n",
    "    except:\n",
    "        return f\"Invalid equation: {equation}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "index can't contain negative values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Main Loop\u001b[39;00m\n\u001b[0;32m      2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/joana/OneDrive/Desktop/HSLU/3rd_semester/CV/c_vision_ocr/data/img_one.png\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# replace with your actual file path\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_equation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m evaluate_equation(predicted_classes)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m, in \u001b[0;36mpredict_equation\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m     width \u001b[38;5;241m=\u001b[39m char\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     21\u001b[0m     filler \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m28\u001b[39m \u001b[38;5;241m-\u001b[39m width) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 22\u001b[0m     char_padded \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mfiller\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     char_arrays\u001b[38;5;241m.\u001b[39mappend(char_padded\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     25\u001b[0m char_arrays \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(char_arrays)\n",
      "File \u001b[1;32mc:\\Users\\joana\\.conda\\envs\\cv\\Lib\\site-packages\\numpy\\lib\\arraypad.py:748\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`pad_width` must be of integral type.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[1;32m--> 748\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m \u001b[43m_as_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(mode):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     function \u001b[38;5;241m=\u001b[39m mode\n",
      "File \u001b[1;32mc:\\Users\\joana\\.conda\\envs\\cv\\Lib\\site-packages\\numpy\\lib\\arraypad.py:518\u001b[0m, in \u001b[0;36m_as_pairs\u001b[1;34m(x, ndim, as_index)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ((x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]),) \u001b[38;5;241m*\u001b[39m ndim\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_index \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain negative values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Converting the array with `tolist` seems to improve performance\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# when iterating and indexing the result (see usage in `pad`)\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbroadcast_to(x, (ndim, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mValueError\u001b[0m: index can't contain negative values"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "image_path = \"C:/Users/joana/OneDrive/Desktop/HSLU/3rd_semester/CV/c_vision_ocr/data/img_one.png\"  # replace with your actual file path\n",
    "\n",
    "predicted_classes = predict_equation(image_path)\n",
    "result = evaluate_equation(predicted_classes)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
